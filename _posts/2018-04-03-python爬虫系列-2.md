---
layout: post
title: python爬虫系列-2
description: 这个是讲述爬虫的系列文章的第二篇,第一篇的例子是一个基本的http请求网页的例子,严格来说还不算是爬虫.这篇文章就讲述了一个基本的爬虫
category: blog
---
![](https://ws3.sinaimg.cn/large/006tNbRwgy1fpzw3twjc8j31kw23vqv5.jpg)
### 1.系列文章列表

[python爬虫系列-1](https://van1988ch.github.io/python%E7%88%AC%E8%99%AB%E7%B3%BB%E5%88%97-1)



###[源码](https://github.com/van1988ch/webrobot/tree/v1.1)


```
#!/usr/bin/env python
# -*- coding: utf8 -*-

__author__ = 'van1988ch'

import urllib2
import re

def Download(url, retrynum=2):
    """抓取网页, retrynum=[500,600)错误重试次数"""
    print 'Downloading:', url
    try:
        html = urllib2.urlopen(url).read()
    except urllib2.URLError as e:
        print 'Download error:', e.reason
        html = None
        if retrynum > 0:
            if hasattr(e, 'code') and 500 <= e.code < 600:
                html = Download(url, retrynum-1)
    except :
        html = None
    return html

def link_crawler(seed_url):
    """分析网页中的内部链接来抓取全部网页
    """
    crawl_queue = [seed_url] 
    while crawl_queue:
        url = crawl_queue.pop()
        html = Download(url)
        if html:
            for link in get_links(html):
                crawl_queue.append(link)


def get_links(html):
    """正则匹配分析出所有的链接
    """
    webpage_regex = re.compile('<a[^>]+href=["\'](.*?)["\']', re.IGNORECASE)
    return webpage_regex.findall(html)

if __name__ == '__main__':
    link_crawler('http://www.iplaysoft.com/')
```


这个爬虫是在第一篇的爬虫的基础上增加了文档正则匹配来获取所有的url连接.然后通过遍历列表递归的抓取所有的页面.本文是讲述爬虫相关内容,如果对于正则[表达式](https://www.liaoxuefeng.com/wiki/001374738125095c955c1e6d8bb493182103fac9270762a000/001386832260566c26442c671fa489ebc6fe85badda25cd000)不是很清楚的可以学习一下.



### 2.缺陷
这个版本严格一样上来说是一个爬虫,但是还是有很多问题没有解决.

- 没有区分相对路径和绝对路径
- 由于计算机请求太快没有做限速处理,很容易被目标主机给屏蔽掉
- 没有对于同一个连接去重
- 由于中国目前的互联网环境有些地方访问不了,爬虫不支持代理
- 没有缓存网页信息

等这些问题,我会在后面的文章和代码中一步步解决这些问题

### 3.总结

这个是一个爬虫的系列文章,这篇文章只是一个最简单的爬虫的代码,我们会采取由浅到深的这个一个方式来写这个系列,希望大家多多关注.[github源代码](https://github.com/van1988ch/webrobot/tree/v1.0)


## 最后

- 项目的代码的github地址[webrobot](https://github.com/van1988ch/webrobot)
- 有什么问题加微博讨论[van1988ch](https://weibo.com/2296015293/profile)
