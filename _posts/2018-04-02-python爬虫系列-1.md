---
layout: post
title: python爬虫系列-1
description: 这个是讲述爬虫的系列文章的第一篇,主要介绍用python实现爬虫需要哪些方面的知识点.使用哪些依赖库来完成爬虫.用其他语言实现也是相同的原理
category: blog
---

##python爬虫当前主要使用的库

###1.builtwith
主要用来查看构建网站使用了哪些技术
安装方法

```
pip install builtwith
```
```
>>> import builtwith
>>> builtwith.parse('http://www.github.com')
{u'web-frameworks': [u'Twitter Bootstrap']}
```

###2.urllib2
想要爬取网页,需要先下载网页的内容,urllib2就是用来抓取网页内容的.

```
>>> urllib2.urlopen("http://www.kuaidi100.com/query?type=quanfengkuaidi&postid=390011492112").read()
'{"message":"\xe5\xbf\xab\xe9\x80\x92\xe5\x85\xac\xe5\x8f\xb8\xe5\x8f\x82\xe6\x95\xb0\xe5\xbc\x82\xe5\xb8\xb8\xef\xbc\x9a\xe5\x8d\x95\xe5\x8f\xb7\xe4\xb8\x8d\xe5\xad\x98\xe5\x9c\xa8\xe6\x88\x96\xe8\x80\x85\xe5\xb7\xb2\xe7\xbb\x8f\xe8\xbf\x87\xe6\x9c\x9f","nu":"","ischeck":"0","condition":"","com":"","status":"201","state":"0","data":[]}'
```

###3.第一个python网络爬虫
下面这10几行代码已经是一个简单的网络爬虫.

```
def Download(url, retrynum=2):
    """抓取网页, retrynum=[500,600)错误重试次数"""
    print 'Downloading:', url
    try:
        html = urllib2.urlopen(url).read()
    except urllib2.URLError as e:
        print 'Download error:', e.reason
        html = None
        if retrynum > 0:
            if hasattr(e, 'code') and 500 <= e.code < 600:
                html = Download(url, retrynum-1)
    return html

Download("http://www.baidu.com")
```

###4.总结
这个是一个爬虫的系列文章,这篇文章只是一个最简单的爬虫的代码,我们会采取由浅到深的这个一个方式来写这个系列,希望大家多多关注.[github源代码](https://github.com/van1988ch/webrobot/tree/v1.0)


## 最后
- 项目的代码的github地址[webrobot](https://github.com/van1988ch/webrobot)
- 有什么问题加微博讨论[van1988ch](https://weibo.com/2296015293/profile)
- 最后介绍一个阿里云的活动,[购买阿里云vps](https://promotion.aliyun.com/ntms/act/group/team.html?group=JdV5A53js1)2018.03.28-04.13 期间，阿里云的新用户「可以在这里参团」以 279 元购买 3 年的云服务器！(或者99/年、189/2年可选)。机器配置同样是 1 核CPU、2G 内存、1M 宽带，但比腾讯云更强的是它配备了 40G 的 SSD 固态硬盘。拿来做测试机搭建wordpress博客还是非常划算的